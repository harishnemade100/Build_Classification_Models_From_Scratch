# Build_Classification_Models_From_Scratch


# Classification Models in Machine Learning ğŸ“Š

This README provides a simple yet complete explanation of major classification algorithms used in Machine Learning.  
Each model includes:
- ğŸ“Œ Explanation
- ğŸ”¢ Formula / Concept
- ğŸ Python Example
- âš–ï¸ Pros & Cons
- ğŸŒ Applications

---

## 1. Logistic Regression

### ğŸ“Œ Explanation
A linear model for **binary classification**. It predicts the probability of belonging to a class using the **sigmoid function**.

### ğŸ”¢ Formula
z = Î²0 + Î²1x1 + Î²2x2 + ... + Î²nxn  
P(y=1|x) = 1 / (1 + e^(-z))

### ğŸ Python Example
```python
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

### âš–ï¸ Pros & Cons
âœ… Simple, interpretable, fast  
âŒ Works only for linear boundaries  

### ğŸŒ Applications
- Spam detection  
- Disease diagnosis  

---

## 2. K-Nearest Neighbors (KNN)

### ğŸ“Œ Explanation
A **non-parametric algorithm** that classifies based on the **majority class of k nearest neighbors**.

### ğŸ”¢ Formula
Distance: Euclidean  
d(x,y) = âˆšÎ£(xi - yi)^2  

### ğŸ Python Example
```python
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=3)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

### âš–ï¸ Pros & Cons
âœ… Simple, no training phase  
âŒ Slow for large datasets, sensitive to noisy data  

### ğŸŒ Applications
- Recommender systems  
- Image recognition  

---

## 3. Decision Tree

### ğŸ“Œ Explanation
A **tree-based model** that splits features based on criteria like **Gini Impurity** or **Entropy**.

### ğŸ”¢ Formula
Entropy = - Î£ p * log2(p)  

### ğŸ Python Example
```python
from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier(criterion="gini")
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

### âš–ï¸ Pros & Cons
âœ… Easy to interpret, handles non-linear data  
âŒ Prone to overfitting  

### ğŸŒ Applications
- Customer churn prediction  
- Loan approval  

---

## 4. Random Forest

### ğŸ“Œ Explanation
An **ensemble of decision trees** using **bagging** to reduce overfitting and improve accuracy.

### ğŸ Python Example
```python
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

### âš–ï¸ Pros & Cons
âœ… High accuracy, robust to noise  
âŒ Slower, less interpretable  

### ğŸŒ Applications
- Fraud detection  
- Healthcare predictions  

---

## 5. Support Vector Machine (SVM)

### ğŸ“Œ Explanation
Finds the **optimal hyperplane** that maximizes the margin between classes.

### ğŸ”¢ Formula
f(x) = wÂ·x + b  
Decision boundary: wÂ·x + b = 0  

### ğŸ Python Example
```python
from sklearn.svm import SVC
model = SVC(kernel="linear")
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

### âš–ï¸ Pros & Cons
âœ… Works well on high-dimensional data  
âŒ Training can be slow for large datasets  

### ğŸŒ Applications
- Face detection  
- Text classification  

---

## 6. Naive Bayes

### ğŸ“Œ Explanation
A **probabilistic classifier** based on **Bayes' theorem**, assuming independence between features.

### ğŸ”¢ Formula
P(y|x) = [P(x|y) * P(y)] / P(x)  

### ğŸ Python Example
```python
from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

### âš–ï¸ Pros & Cons
âœ… Fast, works well with text data  
âŒ Assumes feature independence (not always true)  

### ğŸŒ Applications
- Email spam filtering  
- Sentiment analysis  

---

## 7. Gradient Boosting (XGBoost, LightGBM)

### ğŸ“Œ Explanation
An **ensemble boosting algorithm** that builds trees sequentially, each correcting the previous one.

### ğŸ”¢ Formula
New model = Previous model + Learning rate * Weak learner  

### ğŸ Python Example
```python
from xgboost import XGBClassifier
model = XGBClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

### âš–ï¸ Pros & Cons
âœ… Very powerful, high accuracy  
âŒ Computationally expensive  

### ğŸŒ Applications
- Kaggle competitions ğŸ†  
- Credit scoring  

---

## 8. Neural Networks (Basic)

### ğŸ“Œ Explanation
A network of **neurons (nodes)** organized in layers. Each neuron applies a weighted sum + activation function.

### ğŸ”¢ Formula
z = Î£(wiÂ·xi + b)  
a = Ïƒ(z) (e.g., sigmoid, ReLU)  

### ğŸ Python Example
```python
from sklearn.neural_network import MLPClassifier
model = MLPClassifier(hidden_layer_sizes=(10,10), max_iter=1000)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

### âš–ï¸ Pros & Cons
âœ… Handles complex non-linear data  
âŒ Requires large data, hard to interpret  

### ğŸŒ Applications
- Speech recognition  
- Image classification  

---

# ğŸ“Œ Summary Table

| Model              | Strengths                   | Weaknesses               |
|--------------------|-----------------------------|--------------------------|
| Logistic Regression| Simple, interpretable       | Linear boundaries only   |
| KNN                | Easy, no training           | Slow on big data         |
| Decision Tree      | Intuitive, interpretable    | Overfitting              |
| Random Forest      | Robust, high accuracy       | Less interpretable       |
| SVM                | Good on high-dim data       | Slow for large datasets  |
| Naive Bayes        | Fast, good for text         | Independence assumption  |
| Gradient Boosting  | Very accurate               | Computationally heavy    |
| Neural Networks    | Handles complex patterns    | Needs large data & compute|

---

ğŸ“– This README serves as a **cheat sheet for classification models** in ML.  
